{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "import os\n",
    "from typing import List, Optional, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "from trained_model import ModelRepository\n",
    "from dataset_repo import DatasetRepository\n",
    "\n",
    "\n",
    "def load_model_training_config() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load configuration from .env file.\n",
    "    This is a placeholder - you will implement this function.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with all configuration keys and values\n",
    "    \"\"\"\n",
    "    # Example implementation - replace with your actual .env loading logic\n",
    "    import os\n",
    "    from dotenv import load_dotenv\n",
    "    \n",
    "    load_dotenv()\n",
    "    \n",
    "    config = {\n",
    "        # Database config\n",
    "        'DATABASE_PATH': os.getenv('DATABASE_PATH', '../../data/database.sqlite'),\n",
    "        \n",
    "        # Training config\n",
    "        'TRAIN_RUN_ID': int(os.getenv('TRAIN_RUN_ID', '1')),\n",
    "        'TRAIN_TEST_SIZE': float(os.getenv('TRAIN_TEST_SIZE', '0.2')),\n",
    "        'TRAIN_RANDOM_STATE': int(os.getenv('TRAIN_RANDOM_STATE', '42')),\n",
    "        \n",
    "        # Logistic Regression hyperparameters\n",
    "        'LR_MAX_ITER': int(os.getenv('LR_MAX_ITER', '1000')),\n",
    "        'LR_SOLVER': os.getenv('LR_SOLVER', 'lbfgs'),\n",
    "        'LR_MULTI_CLASS': os.getenv('LR_MULTI_CLASS', 'auto'),\n",
    "        'LR_C': float(os.getenv('LR_C', '1.0')),\n",
    "        'LR_PENALTY': os.getenv('LR_PENALTY', 'l2'),\n",
    "        'LR_TOL': float(os.getenv('LR_TOL', '0.0001')),\n",
    "        'LR_FIT_INTERCEPT': os.getenv('LR_FIT_INTERCEPT', 'True').lower() == 'true',\n",
    "        'LR_WARM_START': os.getenv('LR_WARM_START', 'False').lower() == 'true',\n",
    "        'LR_N_JOBS': int(os.getenv('LR_N_JOBS', '-1')) if os.getenv('LR_N_JOBS') != 'None' else None,\n",
    "        \n",
    "        # Data processing config\n",
    "        'MIN_SAMPLES_PER_CLASS': int(os.getenv('MIN_SAMPLES_PER_CLASS', '1')),\n",
    "        'STRATIFY_SPLIT': os.getenv('STRATIFY_SPLIT', 'True').lower() == 'true',\n",
    "        'SHUFFLE_SPLIT': os.getenv('SHUFFLE_SPLIT', 'True').lower() == 'true',\n",
    "        \n",
    "        # Model storage config\n",
    "        'STORE_COEFFICIENTS_SEPARATELY': os.getenv('STORE_COEFFICIENTS_SEPARATELY', 'False').lower() == 'true',\n",
    "        'FLATTEN_WEIGHTS': os.getenv('FLATTEN_WEIGHTS', 'True').lower() == 'true',\n",
    "    }\n",
    "    \n",
    "    return config\n",
    "\n",
    "def train_model_for_run(run_id) -> dict:\n",
    "    \"\"\"\n",
    "    Train a logistic regression model for a specific run_id using configuration.\n",
    "    \n",
    "    Args:\n",
    "        config: Dictionary containing all configuration values.\n",
    "                If None, loads config using load_model_training_config()\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with training results and model info\n",
    "    \"\"\"\n",
    "    \n",
    "    config = load_model_training_config()\n",
    "    \n",
    "    # =========================================================================\n",
    "    # EXTRACT ALL CONFIGURABLE VALUES\n",
    "    # =========================================================================\n",
    "    \n",
    "    # Database configuration\n",
    "    db_path = config['DATABASE_PATH']\n",
    "\n",
    "    # Training configuration\n",
    "    run_id = config['TRAIN_RUN_ID']\n",
    "    test_size = config['TRAIN_TEST_SIZE']\n",
    "    random_state = config['TRAIN_RANDOM_STATE']\n",
    "    \n",
    "    # Logistic Regression hyperparameters\n",
    "    lr_max_iter = config['LR_MAX_ITER']\n",
    "    lr_solver = config['LR_SOLVER']\n",
    "    lr_multi_class = config['LR_MULTI_CLASS']\n",
    "    lr_c = config['LR_C']\n",
    "    lr_penalty = config['LR_PENALTY']\n",
    "    lr_tol = config['LR_TOL']\n",
    "    lr_fit_intercept = config['LR_FIT_INTERCEPT']\n",
    "    lr_warm_start = config['LR_WARM_START']\n",
    "    lr_n_jobs = config['LR_N_JOBS']\n",
    "    \n",
    "    # Data processing configuration\n",
    "    min_samples_per_class = config['MIN_SAMPLES_PER_CLASS']\n",
    "    stratify_split = config['STRATIFY_SPLIT']\n",
    "    shuffle_split = config['SHUFFLE_SPLIT']\n",
    "    \n",
    "    # Model storage configuration\n",
    "    store_coefficients_separately = config['STORE_COEFFICIENTS_SEPARATELY']\n",
    "    flatten_weights = config['FLATTEN_WEIGHTS']\n",
    "    \n",
    "    # =========================================================================\n",
    "    # LOG CONFIGURATION\n",
    "    # =========================================================================\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"MODEL TRAINING CONFIGURATION\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Database: {db_path}\")\n",
    "    print(f\"Run ID: {run_id}\")\n",
    "    print(f\"Test Size: {test_size}\")\n",
    "    print(f\"Random State: {random_state}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(\"Logistic Regression Hyperparameters:\")\n",
    "    print(f\"  - max_iter: {lr_max_iter}\")\n",
    "    print(f\"  - solver: {lr_solver}\")\n",
    "    print(f\"  - multi_class: {lr_multi_class}\")\n",
    "    print(f\"  - C: {lr_c}\")\n",
    "    print(f\"  - penalty: {lr_penalty}\")\n",
    "    print(f\"  - tol: {lr_tol}\")\n",
    "    print(f\"  - fit_intercept: {lr_fit_intercept}\")\n",
    "    print(f\"  - warm_start: {lr_warm_start}\")\n",
    "    print(f\"  - n_jobs: {lr_n_jobs}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Data Processing: min_samples={min_samples_per_class}, stratify={stratify_split}, shuffle={shuffle_split}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # LOAD SAMPLES\n",
    "    # =========================================================================\n",
    "    \n",
    "    dataset_repo = DatasetRepository(db_path)\n",
    "\n",
    "    print(f\"\\nLoading samples for run_id: {run_id}...\")\n",
    "\n",
    "    samples = dataset_repo.get_run_samples(run_id)\n",
    "    \n",
    "    # Parse features and labels\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        X.append(sample['features'])\n",
    "        y.append(sample['label'])\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Validate classes\n",
    "    unique_classes = np.unique(y)\n",
    "    if len(unique_classes) < 2:\n",
    "        print(y[:10])\n",
    "        raise ValueError(f\"Need at least 2 classes for classification. Found: {unique_classes}\")\n",
    "    \n",
    "    for class_value in unique_classes:\n",
    "        class_count = np.sum(y == class_value)\n",
    "        if class_count < min_samples_per_class:\n",
    "            raise ValueError(\n",
    "                f\"Class {class_value} has only {class_count} samples, \"\n",
    "                f\"minimum required: {min_samples_per_class}\"\n",
    "            )\n",
    "    \n",
    "    print(f\"Features shape: {X.shape}\")\n",
    "    print(f\"Classes: {unique_classes}\")\n",
    "    print(f\"Samples per class: {[np.sum(y == c) for c in unique_classes]}\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    # SPLIT DATA\n",
    "    # =====================================================================\n",
    "    \n",
    "    # Prepare split arguments\n",
    "    split_kwargs = {\n",
    "        'test_size': test_size,\n",
    "        'random_state': random_state,\n",
    "        'shuffle': shuffle_split\n",
    "    }\n",
    "    \n",
    "    # Add stratify if enabled and possible\n",
    "    if stratify_split:\n",
    "        # Check if stratify is possible (all classes have at least 2 samples for test/train)\n",
    "        min_class_samples = min([np.sum(y == c) for c in unique_classes])\n",
    "        if min_class_samples >= 2:\n",
    "            split_kwargs['stratify'] = y\n",
    "        else:\n",
    "            print(f\"Warning: Cannot stratify. Class with minimum samples ({min_class_samples}) < 2\")\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, **split_kwargs)\n",
    "    \n",
    "    print(f\"Training samples: {X_train.shape[0]}\")\n",
    "    print(f\"Testing samples: {X_test.shape[0]}\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    # TRAIN MODEL\n",
    "    # =====================================================================\n",
    "    \n",
    "    print(\"\\nTraining Logistic Regression model...\")\n",
    "        \n",
    "    model = LogisticRegression(\n",
    "        max_iter=lr_max_iter,\n",
    "        random_state=random_state,\n",
    "        solver=lr_solver,\n",
    "        # multi_class=lr_multi_class,\n",
    "        C=lr_c,\n",
    "        penalty=lr_penalty,\n",
    "        tol=lr_tol,\n",
    "        fit_intercept=lr_fit_intercept,\n",
    "        warm_start=lr_warm_start,\n",
    "        n_jobs=lr_n_jobs\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # =====================================================================\n",
    "    # EVALUATE MODEL\n",
    "    # =====================================================================\n",
    "    \n",
    "    train_accuracy = model.score(X_train, y_train)\n",
    "    test_accuracy = model.score(X_test, y_test)\n",
    "    \n",
    "    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    # EXTRACT WEIGHTS\n",
    "    # =====================================================================\n",
    "    \n",
    "    weights = []\n",
    "    \n",
    "    if store_coefficients_separately:\n",
    "        # Store as separate arrays (for future implementation)\n",
    "        coefficients = model.coef_.tolist() if hasattr(model, 'coef_') else []\n",
    "        intercept = model.intercept_.tolist() if hasattr(model, 'intercept_') else []\n",
    "        \n",
    "        # You can modify ModelRepository to handle separate storage\n",
    "        # For now, we'll still flatten for backward compatibility\n",
    "        if flatten_weights:\n",
    "            for class_idx in range(len(coefficients)):\n",
    "                weights.extend(coefficients[class_idx])\n",
    "            weights.extend(intercept)\n",
    "    else:\n",
    "        # Flatten weights for storage\n",
    "        if flatten_weights and hasattr(model, 'coef_'):\n",
    "            for class_idx in range(model.coef_.shape[0]):\n",
    "                weights.extend(model.coef_[class_idx].tolist())\n",
    "            \n",
    "            if hasattr(model, 'intercept_'):\n",
    "                weights.extend(model.intercept_.tolist())\n",
    "    \n",
    "    print(f\"Number of weights extracted: {len(weights)}\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    # SAVE MODEL\n",
    "    # =====================================================================\n",
    "    \n",
    "    repo = ModelRepository(db_path)\n",
    "    model_id = repo.save_model(\n",
    "        run_id=run_id,\n",
    "        weights=weights,\n",
    "        accuracy=float(test_accuracy)\n",
    "    )\n",
    "    \n",
    "    print(f\"Model saved with ID: {model_id}\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    # RETURN RESULTS\n",
    "    # =====================================================================\n",
    "    \n",
    "    return {\n",
    "        'run_id': run_id,\n",
    "        'model_id': model_id,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'weights_count': len(weights),\n",
    "        'features_shape': X.shape,\n",
    "        'n_samples': len(X),\n",
    "        'n_classes': len(unique_classes),\n",
    "        'class_distribution': {int(c): int(np.sum(y == c)) for c in unique_classes},\n",
    "        'config_used': {\n",
    "            'db_path': db_path,\n",
    "            'test_size': test_size,\n",
    "            'random_state': random_state,\n",
    "            'lr_max_iter': lr_max_iter,\n",
    "            'lr_solver': lr_solver,\n",
    "            'lr_multi_class': lr_multi_class,\n",
    "            'lr_c': lr_c,\n",
    "            'lr_penalty': lr_penalty,\n",
    "            'stratify_used': 'stratify' in split_kwargs\n",
    "        },\n",
    "        'model_info': {\n",
    "            'coef_shape': model.coef_.shape if hasattr(model, 'coef_') else None,\n",
    "            'intercept_shape': model.intercept_.shape if hasattr(model, 'intercept_') else None,\n",
    "            'classes': model.classes_.tolist() if hasattr(model, 'classes_') else None,\n",
    "            'n_features_in_': getattr(model, 'n_features_in_', None)\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL TRAINING CONFIGURATION\n",
      "============================================================\n",
      "Database: ../../data/database.sqlite\n",
      "Run ID: 1\n",
      "Test Size: 0.2\n",
      "Random State: 42\n",
      "------------------------------------------------------------\n",
      "Logistic Regression Hyperparameters:\n",
      "  - max_iter: 1000\n",
      "  - solver: lbfgs\n",
      "  - multi_class: auto\n",
      "  - C: 1.0\n",
      "  - penalty: l2\n",
      "  - tol: 0.0001\n",
      "  - fit_intercept: True\n",
      "  - warm_start: False\n",
      "  - n_jobs: -1\n",
      "------------------------------------------------------------\n",
      "Data Processing: min_samples=1, stratify=True, shuffle=True\n",
      "============================================================\n",
      "\n",
      "Loading samples for run_id: 1...\n",
      "Features shape: (10506, 24)\n",
      "Classes: [1 2 3 4]\n",
      "Samples per class: [np.int64(3610), np.int64(690), np.int64(4497), np.int64(1709)]\n",
      "Training samples: 8404\n",
      "Testing samples: 2102\n",
      "\n",
      "Training Logistic Regression model...\n",
      "Training Accuracy: 1.0000\n",
      "Testing Accuracy: 1.0000\n",
      "Number of weights extracted: 100\n",
      "Model saved with ID: 1\n",
      "{'run_id': 1, 'model_id': 1, 'test_accuracy': 1.0, 'train_accuracy': 1.0, 'weights_count': 100, 'features_shape': (10506, 24), 'n_samples': 10506, 'n_classes': 4, 'class_distribution': {1: 3610, 2: 690, 3: 4497, 4: 1709}, 'config_used': {'db_path': '../../data/database.sqlite', 'test_size': 0.2, 'random_state': 42, 'lr_max_iter': 1000, 'lr_solver': 'lbfgs', 'lr_multi_class': 'auto', 'lr_c': 1.0, 'lr_penalty': 'l2', 'stratify_used': True}, 'model_info': {'coef_shape': (4, 24), 'intercept_shape': (4,), 'classes': [1, 2, 3, 4], 'n_features_in_': 24}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "m:\\projects\\ML Duel Game\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "m:\\projects\\ML Duel Game\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "run_id = 1\n",
    "    \n",
    "print(train_model_for_run(run_id))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
